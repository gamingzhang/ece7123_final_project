{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjppAWOfKne8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7KrKMZ6GUCL"
      },
      "outputs": [],
      "source": [
        "# Configuration for evolution algorithm\n",
        "\n",
        "population_size = 20  # population size\n",
        "numbers_of_generation = 10  # numbers of generation\n",
        "init_epochs = 10  # numbers of epochs we train the model during the evolution\n",
        "fully_epochs = 200  # numbers of epochs we train the best model\n",
        "part1_min = 7  # minimal size of part1\n",
        "part1_max = 15  # maximum size of part1\n",
        "kernel_max = 256  # maximum kernel for conv layer\n",
        "add_conv_probability = 0.5  # The probability of adding conv instead of pool during the initialization\n",
        "add_max_pool_probability = 0.8  # The probability of adding maxPool instead of AvgPool during the initialization\n",
        "mutation_size = population_size // 2  # numbers of new individuals after mutation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuD9hEsA6b-W",
        "outputId": "c326f237-92d0-4f8a-d140-b52445800ea5"
      },
      "outputs": [],
      "source": [
        "# Define hyper-parameters for training\n",
        "batch_size = 64\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvGRngGpdrvP"
      },
      "outputs": [],
      "source": [
        "def train_model(epochs, net):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "    net = net.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Epoch: {}/{}, Test Accuracy: {:.2f}'.format(epoch + 1, epochs, correct / total))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2F3Nfp1C6-o"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "0 - Conv2d\n",
        "1 - ReLU\n",
        "2 - MaxPool2d\n",
        "3 - AvgPool2d\n",
        "4 - Linear\n",
        "5 - Dropout\n",
        "'''\n",
        "\n",
        "\n",
        "class Individual:\n",
        "\n",
        "    def __init__(self, part1, part2):\n",
        "        # Part1 only contains Conv, Pool, and ReLU\n",
        "        self.part1 = part1\n",
        "        # Part2 only contains Linear, Dropout, and ReLU\n",
        "        self.part2 = part2\n",
        "        self.score = {\"accuracy\": 0, \"parameters\": 0}\n",
        "\n",
        "    def fill_score(self):\n",
        "        torch.cuda.empty_cache()\n",
        "        model = self.decodeCNN()\n",
        "        self.score['accuracy'] = train_model(init_epochs, model)\n",
        "        # self.score['accuracy'] = random.randint(1, 99)\n",
        "        self.score['parameters'] = count_parameters(model)\n",
        "        model = model.to(device)\n",
        "        # summary(model, (3, 32, 32))\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def transferToLayer(self, args):\n",
        "        # After conv, the image size remains the same\n",
        "        mapping = {\n",
        "            1: 0,  # 1x1 kernel size -> padding = 0\n",
        "            3: 1,  # 3x3 kernel size -> padding = 1\n",
        "            5: 2,  # 5x5 kernel size -> padding = 2\n",
        "            7: 3,  # 7x7 kernel size -> padding = 3\n",
        "        }\n",
        "        layer_type = args[0]\n",
        "        if layer_type == 0:\n",
        "            return nn.Conv2d(in_channels=args[1], out_channels=args[2], kernel_size=args[3], padding=mapping[args[3]])\n",
        "        elif layer_type == 1:\n",
        "            return nn.ReLU()\n",
        "        elif layer_type == 2:\n",
        "            return nn.MaxPool2d(2, 2)\n",
        "        elif layer_type == 3:\n",
        "            return nn.AvgPool2d(2, 2)\n",
        "        elif layer_type == 4:\n",
        "            return nn.Linear(args[1], args[2])\n",
        "        elif layer_type == 5:\n",
        "            return nn.Dropout()\n",
        "\n",
        "    def decodeCNN(self):\n",
        "        return nn.Sequential(\n",
        "            *map(self.transferToLayer, self.part1),\n",
        "            nn.Flatten(),\n",
        "            *map(self.transferToLayer, self.part2),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd6UjgD6e1N1"
      },
      "outputs": [],
      "source": [
        "def get_random_kernel_size():\n",
        "    # we only use three types of kernels\n",
        "    # 1x1 3x3 5x5 7x7\n",
        "    return random.choice([1, 3, 5, 7])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbIQf7I2CWNL"
      },
      "outputs": [],
      "source": [
        "def initialization(size):\n",
        "    population = []\n",
        "\n",
        "    for _ in range(size):\n",
        "        part1 = []\n",
        "        part2 = []\n",
        "        output_size = 32  # CIFAR10: 32x32\n",
        "        p1_length = random.randint(part1_min, part1_max)\n",
        "\n",
        "        # First layer must be conv\n",
        "        prev_out_channels = random.randint(5, 12)\n",
        "        part1.extend([\n",
        "            (0, 3, prev_out_channels, get_random_kernel_size()),  # Conv\n",
        "            (1, )  # ReLU\n",
        "        ])\n",
        "\n",
        "        # Part1\n",
        "        for _ in range(2, p1_length + 1):\n",
        "            if random.random() < add_conv_probability:\n",
        "                # Add Conv\n",
        "                cur_out_channels = random.randint(prev_out_channels, kernel_max)\n",
        "                part1.extend([(0, prev_out_channels, cur_out_channels, get_random_kernel_size()), (1, )])\n",
        "                prev_out_channels = cur_out_channels\n",
        "            else:\n",
        "                # Add Pool\n",
        "                # We can only add a maximum of 4 pool layers\n",
        "                # For each pooling layer, the image size is reduced to half of the original size\n",
        "                # 32 -> 16 -> 8 -> 4 -> 2\n",
        "                if output_size == 2:\n",
        "                    continue\n",
        "                if random.random() < add_max_pool_probability:  # TODO 50% add maxPool, 50% add avgPool\n",
        "                    # add MaxPool\n",
        "                    part1.append((2, ))\n",
        "                else:\n",
        "                    # AvgPool\n",
        "                    part1.append((3, ))\n",
        "                output_size //= 2\n",
        "\n",
        "        # Part2\n",
        "        prev_features = prev_out_channels * output_size * output_size\n",
        "\n",
        "        part2.extend([\n",
        "            (5, ),\n",
        "            (4, prev_features, max(10, prev_features // 2)),\n",
        "            (1, ),\n",
        "            (5, ),\n",
        "            (4, max(10, prev_features // 2), max(10, prev_features // 4)),\n",
        "            (1, ),\n",
        "            (4, max(10, prev_features // 4), 10),\n",
        "        ])\n",
        "\n",
        "        population.append(Individual(part1, part2))\n",
        "\n",
        "    for individual in population:\n",
        "        individual.fill_score()\n",
        "\n",
        "    return population\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOWyJrxBGPBy"
      },
      "outputs": [],
      "source": [
        "from itertools import zip_longest\n",
        "\n",
        "\n",
        "def mutation(population):\n",
        "    best = 5\n",
        "    next_generation = []\n",
        "    part1Set = set()\n",
        "\n",
        "    while len(next_generation) < mutation_size:\n",
        "        best_idx = random.randint(0, best - 1)\n",
        "        best_individual = population[best_idx]\n",
        "\n",
        "        r1 = random.randint(0, len(population) - 1)\n",
        "        while r1 == best_idx:\n",
        "            r1 = random.randint(0, len(population) - 1)\n",
        "\n",
        "        r2 = random.randint(0, len(population) - 1)\n",
        "        while r2 in (r1, best_idx):\n",
        "            r2 = random.randint(0, len(population) - 1)\n",
        "\n",
        "        r1_minus_r2 = differentiate(population[r1], population[r2])\n",
        "        new_individual = merge(best_individual, r1_minus_r2)\n",
        "        if tuple(new_individual.part1) in part1Set:\n",
        "            continue\n",
        "        part1Set.add(tuple(new_individual.part1))\n",
        "        next_generation.append(new_individual)\n",
        "\n",
        "    for individual in next_generation:\n",
        "        individual.fill_score()\n",
        "\n",
        "    return next_generation\n",
        "\n",
        "\n",
        "def differentiate(ind1, ind2):\n",
        "    add_to_best = []\n",
        "    p1 = p2 = 0\n",
        "    # Remove ReLU\n",
        "    seq1 = [x for x in ind1.part1 if x[0] != 1]\n",
        "    seq2 = [x for x in ind2.part1 if x[0] != 1]\n",
        "\n",
        "    while p1 < len(seq1) or p2 < len(seq2):\n",
        "        if p1 == len(seq1):\n",
        "            add_to_best.append(seq2[p2])\n",
        "            p2 += 1\n",
        "            continue\n",
        "        if p2 == len(seq2):\n",
        "            add_to_best.append(seq1[p1])\n",
        "            p1 += 1\n",
        "            continue\n",
        "        if seq1[p1][0] == seq2[p2][0]:\n",
        "            add_to_best.append((-1, ))  # -1 represents \"remain the same\"\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        else:\n",
        "            add_to_best.append(seq1[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "\n",
        "    return add_to_best\n",
        "\n",
        "\n",
        "def merge(best_individual, r1_minus_r2):\n",
        "    new_part1 = []\n",
        "    seq = [x for x in best_individual.part1 if x[0] != 1]  # Remove ReLU\n",
        "    count_pool = 0\n",
        "    p1 = p2 = 0\n",
        "    in_channels = 3\n",
        "\n",
        "    while p1 < len(seq):\n",
        "        if p2 == len(r1_minus_r2):\n",
        "            new_part1.append(seq[p1])\n",
        "            p1 += 1\n",
        "        elif r1_minus_r2[p2][0] == -1:\n",
        "            new_part1.append(seq[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        else:\n",
        "            new_part1.append(r1_minus_r2[p2])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "\n",
        "        if new_part1[-1][0] == 0:\n",
        "            replace = (new_part1[-1][0], in_channels, new_part1[-1][2], new_part1[-1][3])\n",
        "            in_channels = replace[2]\n",
        "            new_part1.pop()\n",
        "            new_part1.extend([replace, (1, )])\n",
        "        elif new_part1[-1][0] in (2, 3):\n",
        "            count_pool += 1\n",
        "            if count_pool > 4:\n",
        "                new_part1.pop()\n",
        "                count_pool -= 1\n",
        "\n",
        "    features = int(in_channels * (32 * (1 / 2)**count_pool) * (32 * (1 / 2)**count_pool))\n",
        "    new_part2 = [\n",
        "        (5, ),\n",
        "        (4, features, max(10, features // 2)),\n",
        "        (1, ),\n",
        "        (5, ),\n",
        "        (4, max(10, features // 2), max(10, features // 4)),\n",
        "        (1, ),\n",
        "        (4, max(10, features // 4), 10),\n",
        "    ]\n",
        "\n",
        "    return Individual(new_part1, new_part2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj79pV7GIy9t"
      },
      "outputs": [],
      "source": [
        "def get_score(err_max, err_min, para_max, para_min, err, para):\n",
        "    # Normalization\n",
        "    err = (err - err_min) / (err_max - err_min)\n",
        "    para = (para - para_min) / (para_max - para_min)\n",
        "    score = 0.7 * err + 0.3 * para\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOPz--iiEMkT"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "population = initialization(population_size)\n",
        "err_max = 1 - min(x.score['accuracy'] for x in population)\n",
        "err_min = 1 - max(x.score['accuracy'] for x in population)\n",
        "para_max = max(x.score['parameters'] for x in population)\n",
        "para_min = min(x.score['parameters'] for x in population)\n",
        "population.sort(key=lambda x: get_score(err_max, err_min, para_max, para_min, 1 - x.score['accuracy'], x.score['parameters']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI5L0qqDENJu"
      },
      "outputs": [],
      "source": [
        "for g in range(2, numbers_of_generation):\n",
        "    # Mutation\n",
        "    new_population = mutation(population)\n",
        "    population.extend(new_population)\n",
        "    err_max = 1 - min(x.score['accuracy'] for x in population)\n",
        "    err_min = 1 - max(x.score['accuracy'] for x in population)\n",
        "    para_max = max(x.score['parameters'] for x in population)\n",
        "    para_min = min(x.score['parameters'] for x in population)\n",
        "    population.sort(key=lambda x: get_score(err_max, err_min, para_max, para_min, 1 - x.score['accuracy'], x.score['parameters']))\n",
        "    population = population[:population_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "4yJGigYgpZr5",
        "outputId": "0305c8ea-8c0e-47a1-dcc1-bc014e771e86"
      },
      "outputs": [],
      "source": [
        "# fully train the best CNN\n",
        "best_individual = population[0]\n",
        "train_model(fully_epochs, best_individual.decodeCNN())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
